# -*- coding: utf-8 -*-
"""submission_sistem_rekomendasi_film.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SICzpa16WXxCrGJJlQwqZSEmYRT9aLrX

# **Proyek Machine learning-Andry Septian Syahputra Tumaruk-MC476D5Y0692-MC39**

# Import Library
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import ast

"""Pada kode diatas ini kita akan mengimport library yang dibutuhkan untuk menjalankan proyek ini

# Load data
"""

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d 'tmdb/tmdb-movie-metadata'

"""- **Sumber Data**: Dataset [TMDB Movie Metadata](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata) dari Kaggle.
- **Proses**:
  - Menggunakan API Kaggle untuk mengunduh dataset.
  - Membaca file CSV (`tmdb_5000_movies.csv` dan `tmdb_5000_credits.csv`) ke dalam DataFrame.
"""

!unzip tmdb-movie-metadata.zip
# --- Tahap 1: Memuat dataset film ---
movies_df = pd.read_csv('tmdb_5000_movies.csv')
credits_df = pd.read_csv('tmdb_5000_credits.csv')

print('jumlah data pada movies:',len(movies_df.id.unique()))
print('jumlah data pada credits:',len(credits_df.movie_id.unique()))

"""sintak diatas digunakan untuk memvalidasi jumlah data unik untuk memastikan konsistensi.

# Univariate Exploratory Data Analysis
"""

movies_df.info()
movies_df.head()

"""- beberapa kolom yang memiliki missing value seperti `homepage`,`release_date`,`runtime`,dan `tagline`
- Tipe data didominasi oleh object: 13,integer: 4, dan Float: 3.
- Memori yang digunakan sekitar 750.6+ KB untuk file `tmdb_5000_movies.csv`.
- menampilkan 5 data paling atas.
"""

print('banyak data:',len(movies_df.id.unique()))
print('banyak jenis genre:',len(movies_df))
print('jenis status film:',movies_df.status.unique())
print('banyak vote dilakukan:',len(movies_df.vote_count.unique()))

"""- **Jumlah Data (Baris):** Terdapat 4803 entri atau baris data dalam dataset movies_df. Ini ditunjukkan oleh len(movies_df.id.unique()) dan len(movies_df). Menariknya, jumlah ID unik sama dengan jumlah total baris, yang mengindikasikan bahwa setiap film memiliki ID yang unik dan tidak ada duplikasi berdasarkan ID.

- **Variasi Genre:** Meskipun ada 4803 baris data, len(movies_df) yang juga menghasilkan 4803 untuk "banyak jenis genre" menunjukkan bahwa setiap entri dalam dataset bisa jadi merepresentasikan satu genre atau ada entri duplikat yang dihitung sebagai "jenis genre" yang berbeda jika kolom genre tidak ditangani dengan benar (misalnya, jika genre adalah string yang berisi beberapa genre dipisahkan koma dan tidak dipecah). Jika "banyak jenis genre" dimaksudkan untuk menghitung genre unik, maka kode tersebut salah karena hanya menghitung jumlah baris. Namun, jika movies_df adalah DataFrame di mana setiap baris sudah merupakan entri untuk satu film (terlepas dari berapa banyak genre yang dimiliki satu film), dan pertanyaan "banyak jenis genre" sebenarnya ingin tahu berapa banyak film yang ada, maka hasilnya konsisten dengan jumlah data. Untuk mendapatkan jumlah genre unik, kita perlu melihat bagaimana kolom genre disimpan.

- **Status Film:** Ada 3 jenis status film yang berbeda dalam dataset: 'Released', 'Post Production', dan 'Rumored'. Ini menunjukkan bahwa dataset mencakup film dalam berbagai tahap siklus produksinya.

- **Variasi Jumlah Vote:** Terdapat 1609 nilai unik untuk kolom vote_count. Ini berarti bahwa meskipun ada 4803 film, tidak semua film memiliki jumlah vote yang berbeda. Ada beberapa film yang memiliki jumlah vote yang sama.
"""

print('banyak film di produksi dari berbagai perusahaan:',len(movies_df.production_companies.unique()))
print('jenis perusahaan yang memproduksi film:',movies_df.production_companies.unique())

"""- dapat diketahui bahwa produksi film dari berbagai perusahaan itu sebanyak 3697 film

- mengetahui jenis perusahaan apa saja yang memproduksi film
"""

print('banyak film yang di produksi dari berbagai negara:',len(movies_df.production_companies.unique()))
print('banyak film yang diproduksi dari berbagai negara',movies_df.production_countries.value_counts())

"""- dapat diketahui bahwa produksi film dari berbagai negara itu sebanyak 3697 film

- mengetahui negara apa saja yang memproduksi sebuah film
"""

credits_df.info()
credits_df.head()

"""- Semua kolom memiliki jumlah data lengkap (non-null), artinya tidak ada missing value yang perlu ditangani saat ini.
- Tipe data didominasi oleh object,bertipe dan integer.
- Memori yang digunakan sekitar 150 KB.
"""

print('banyak judul film:',len(credits_df.title.unique()))
print('jumlah cast:',len(credits_df.cast.unique()))
print('jumlah crew:',len(credits_df.crew.unique()))

"""- mengetahui jumlah judul film
- terdapat 4803 data cast dan saat dijumlah ada 4761 cast.artinya ada beberapa cast yang sama main di film yang berbeda
- terdapat 4083 data crew dan saat di jumlah ada 4776 crew.artinya ada beberapa crew yang sama bekerja di film yang berbeda

# Data preprocessing
"""

#tahap 2:Menggabungkan kedua dataframe berdasarkan 'id' film (atau 'movie_id' di credits_df dan 'id' di movies_df)
movies_df = movies_df.rename(columns={'id': 'movie_id'}) # Menyamakan nama kolom untuk merge
df = movies_df.merge(credits_df, on='movie_id')
df.info()

"""- kode diatas mengubah kolom id menjadi movie_id di variabel movie_df
- menggabungkan kedua data diatas menjadi data dengan menggunakan fungsi merge  
"""

# Memilih fitur yang relevan untuk content-based filtering
# Kita akan menggunakan 'overview', 'genres', 'keywords', 'cast', dan 'crew' (sutradara)
df = df[['movie_id', 'title_x', 'overview', 'genres', 'keywords', 'cast', 'crew']]
df.rename(columns={'title_x': 'title'}, inplace=True)

print("Dataset Gabungan (beberapa kolom pertama):")
print(df.head(2))
print(f"\nJumlah film awal: {len(df)}")
print("-" * 30)

"""- bertujuan untuk mempersiapkan data untuk content-based filtering
-  Fitur-fitur yang dipilih untuk content-based filtering adalah: `overview`,` genres`,`keywords`,`cast`,dan `crew`
- Kolom title_x diubah namanya menjadi title

# Data Preparation
"""

# --- Tahap 3: Pra-pemrosesan Data ---
print("Tahap 3: Pra-pemrosesan Data:")

# Menangani nilai yang hilang (missing values)
df.isnull().sum()

"""- melihat data yang hilang"""

df.dropna(inplace=True) # Menghapus baris dengan nilai NaN pada kolom overview
print(f"Jumlah film setelah menghapus NaN: {len(df)}")

"""- menangani data yang hilang dan melihat jumlah data yang ada sebanyak 4800"""

# --- Tahap 4: Feature Engineering ---
# Fungsi untuk mengurai string JSON menjadi list
def parse_json_like_string(text_data):
    try:
        # ast.literal_eval aman untuk mengurai string yang terlihat seperti struktur data Python
        items = ast.literal_eval(text_data)
        # Ambil 'name' dari setiap dictionary dalam list
        return [item['name'] for item in items]
    except:
        return []

# Mengaplikasikan fungsi parsing ke kolom 'genres' dan 'keywords'
df['genres'] = df['genres'].apply(parse_json_like_string)
df['keywords'] = df['keywords'].apply(parse_json_like_string)

# Fungsi untuk mendapatkan 3 aktor utama dari kolom 'cast'
def get_top_n_actors(text_data, n=3):
    try:
        cast_list = ast.literal_eval(text_data)
        # Ambil 'name' dari n aktor pertama, urutkan berdasarkan 'order' jika ada
        # Di sini kita ambil berdasarkan urutan kemunculan saja
        return [actor['name'] for actor in cast_list[:n]]
    except:
        return []

df['cast'] = df['cast'].apply(get_top_n_actors)

"""- **Tujuan Fungsi**: Fungsi parse_json_like_string(text_data) dirancang khusus untuk "mengurai string JSON menjadi list". Ini mengonfirmasi dugaan sebelumnya bahwa kolom-kolom seperti genres, keywords, cast, dan crew yang sebelumnya terlihat seperti string JSON perlu diuraikan.

- **Metode Penguraian (Parsing):**

 - Fungsi ini menggunakan `ast.literal_eval()`. ast.literal_eval mengevaluasi ekspresi Python yang valid, tidak mengeksekusi kode arbitrer.
 - Fungsi ini bertujuan untuk mengubah string yang merepresentasikan struktur data Python (seperti daftar kamus) menjadi objek Python yang sebenarnya.

- **Ekstraksi Informasi Spesifik:**

 - Setelah string diuraikan menjadi `items` (yang diharapkan menjadi list of dictionaries), kode mengambil nilai dari kunci `'name'` dari setiap kamus dalam list tersebut (`return [item['name'] for item in items]`). Ini sangat relevan untuk kolom seperti `genres` dan `keywords` di mana yang kita butuhkan hanyalah nama genre atau kata kunci. Untuk `cast` dan `crew`, ini akan mengekstrak nama aktor atau kru.

"""

# Fungsi untuk mendapatkan sutradara dari kolom 'crew'
def get_director(text_data):
    try:
        crew_list = ast.literal_eval(text_data)
        for member in crew_list:
            if member['job'] == 'Director':
                return [member['name']]
        return []
    except:
        return []
df['crew'] = df['crew'].apply(get_director)

"""- **Parsing String:** Sama seperti fungsi sebelumnya, fungsi ini menggunakan `ast.literal_eval(text_data)` untuk mengurai string `crew` yang berisi struktur JSON (list of dictionaries) menjadi objek list Python.

- **Iterasi dan Pencarian:** Fungsi kemudian mengiterasi melalui setiap `member` dalam `crew_list`.

- **Kondisi Spesifik:** Ia mencari anggota kru di mana nilai `member['job']` sama dengan `'Director'`.

- **Pengembalian Nama:** Jika 'Director' ditemukan, fungsi segera mengembalikan nama sutradara dalam bentuk list (`[member['name']]`). Penting untuk dicatat bahwa ini mengembalikan hanya sutradara pertama yang ditemukan jika ada beberapa entri 'Director'. Mengembalikan dalam bentuk list `[]` menunjukkan konsistensi dengan format keluaran fitur lain (misalnya `genres`, `keywords`) yang juga berupa list.

- Baris terakhir df['crew'] = df['crew'].apply(get_director) menunjukkan bagaimana fungsi get_director diterapkan ke seluruh kolom crew dari DataFrame df.
"""

# Membersihkan spasi dalam nama (misal 'Science Fiction' -> 'ScienceFiction')
# agar tidak dianggap sebagai dua kata terpisah oleh TfidfVectorizer
def clean_spaces(item_list):
    return [str(i).replace(" ", "").lower() for i in item_list]

for feature in ['genres', 'keywords', 'cast', 'crew']:
    df[feature] = df[feature].apply(clean_spaces)

"""- **Tujuan:** Fungsi `clean_spaces(item_list)` dirancang untuk menghapus spasi dari nama-nama (misalnya, "Science Fiction" menjadi "ScienceFiction").

- Fungsi ini diterapkan ke kolom genres, keywords, cast, dan crew. Ini menunjukkan bahwa semua fitur ini akan diolah agar frasa seperti "Tom Hanks" menjadi "tomhanks" dan "Action" menjadi "action".
"""

# Menggabungkan fitur-fitur teks menjadi satu 'tag' untuk setiap film
# 'overview' diubah menjadi list kata agar bisa digabungkan
df['overview'] = df['overview'].apply(lambda x: str(x).split())

"""- Kolom overview diubah menjadi list kata dengan menggunakan str(x).split(). Ini berarti setiap sinopsis akan dipecah menjadi daftar kata-kata individu. Ini adalah langkah standar untuk memproses teks bebas agar siap untuk analisis lebih lanjut (misalnya, TF-IDF)."""

# Membuat kolom 'tags' yang merupakan gabungan dari overview, genres, keywords, cast, dan crew
df['tags'] = df['overview'] + df['genres'] + df['keywords'] + df['cast'] + df['crew']

"""- Penggabungan Fitur: Ini adalah langkah feature engineering yang sangat penting. Kolom tags dibuat dengan menggabungkan konten dari overview, genres, keywords, cast, dan crew.

- Tujuan: Menggabungkan semua fitur ini ke dalam satu kolom tags menciptakan representasi teks tunggal yang kaya untuk setiap film, yang akan menjadi dasar untuk content-based filtering. Ini memungkinkan model untuk menemukan kesamaan antar film berdasarkan gabungan dari semua elemen ini.
"""

# Menggabungkan list kata dalam 'tags' menjadi satu string per film
df['tags'] = df['tags'].apply(lambda x: " ".join(x).lower()) # gabungkan dan ubah ke lowercase

"""- Setelah tags menjadi daftar dari daftar (list of lists, karena penggabungan kolom yang sudah berupa list), langkah selanjutnya adalah menggabungkan semua kata dalam list tags menjadi satu string tunggal per film.
- Ini dilakukan dengan lambda x: " ".join(x).lower(), yang menggabungkan semua elemen dalam list x dengan spasi di antaranya dan mengubahnya menjadi huruf kecil. Ini menghasilkan string "tag" akhir yang bersih dan siap untuk vektorisasi teks.
"""

# Membuat dataframe baru yang hanya berisi 'movie_id', 'title', dan 'tags'
final_df = df[['movie_id', 'title', 'tags']]

"""- final_df dibuat, hanya berisi kolom movie_id, title, dan tags. Ini menunjukkan bahwa semua fitur lain yang telah diolah dan digabungkan kini direpresentasikan dalam kolom tags, dan hanya tiga kolom ini yang diperlukan untuk tahap berikutnya (misalnya, pembuatan matriks kesamaan dan rekomendasi)."""

print("\nContoh 'tags' untuk film pertama:")
print(final_df['tags'].iloc[0][:500] + "...") # Tampilkan 500 karakter pertama
print("-" * 30)

"""- Baris print(final_df['tags'].iloc[0][:500] + "...") digunakan untuk menampilkan 500 karakter pertama dari kolom tags untuk film pertama. Ini adalah langkah yang baik untuk memverifikasi format dan isi dari kolom tags yang telah diproses."""

data= final_df
data.sample(5)

"""- mengambil 5 data sample secara random dari variabel data yang di simpan dalam variabel final_df

## TF-IDF Vectorizer
"""

# --- Tahap 5: Feature Extraction (TF-IDF) ---
print("Tahap 5: Feature Extraction (TF-IDF)...")

# Membuat TfidfVectorizer
# max_features: batasi jumlah fitur yang digunakan untuk efisiensi
# stop_words: hilangkan kata-kata umum
tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')

# Membuat matriks TF-IDF dari kolom 'tags'
tfidf_matrix = tfidf_vectorizer.fit_transform(data['tags'])

print("\nUkuran matriks TF-IDF:", tfidf_matrix.shape)
# tfidf_matrix.shape akan menjadi (jumlah film, max_features)
print("-" * 30)

"""1. **Menginisialisasi `TfidfVectorizer`**:

 - Tujuannya adalah untuk menyiapkan alat yang akan mengubah teks menjadi representasi numerik.

 - Parameter `max_features=5000` bertujuan untuk membatasi kompleksitas model dan komputasi dengan hanya mempertimbangkan 5000 kata yang paling relevan atau sering muncul. Ini penting untuk efisiensi dan untuk menghindari masalah "dimensi tinggi" (high dimensionality) yang bisa memperlambat proses atau bahkan menurunkan performa model.

 - Parameter `stop_words='english'` bertujuan untuk menghilangkan kata-kata umum dalam bahasa Inggris (seperti "dan", "adalah", "yang", dll.) yang sering muncul tetapi tidak banyak memberikan informasi diskriminatif tentang konten sebuah film. Menghilangkannya membantu fokus pada kata-kata yang lebih bermakna.

2. **Membuat Matriks TF-IDF dari Kolom `tags`**:

 - Tujuannya adalah untuk mengubah data teks (dari kolom tags yang berisi gabungan overview, genres, keywords, cast, dan crew) menjadi format numerik.
 - `tfidf_vectorizer.fit_transform(data['tags'])` melakukan dua hal penting:
    - `fit`: Mempelajari kosakata unik dari semua "tags" film dan menetapkan indeks numerik untuk setiap kata.
    - `transform`: Menghitung skor TF-IDF untuk setiap kata dalam setiap film. Skor ini merefleksikan seberapa sering sebuah kata muncul dalam sebuah film (Term Frequency) dikalikan dengan seberapa jarang kata tersebut muncul di seluruh koleksi film (Inverse Document Frequency). Semakin tinggi skor TF-IDF suatu kata, semakin penting kata tersebut untuk mendefinisikan konten film tersebut di antara film-film lainnya.
 - Hasilnya adalah `tfidf_matrix`, sebuah matriks di mana setiap baris mewakili satu film, dan setiap kolom mewakili sebuah kata unik dengan nilai TF-IDF-nya.

3. Menampilkan Ukuran Matriks TF-IDF:

- Tujuannya adalah untuk memverifikasi dimensi dari matriks yang dihasilkan (`tfidf_matrix.shape`). Ini memberikan informasi tentang jumlah film (baris) dan jumlah fitur/kata yang dipertahankan (kolom), memastikan bahwa proses vektorisasi berjalan sesuai harapan dan memberikan gambaran tentang ukuran data yang akan diproses selanjutnya.

# Model Development dengan Content Based Filtering

## cosine similaritas
"""

# --- Tahap 6: Menghitung Similaritas Cosine ---
print("Tahap 6: Menghitung Similaritas Cosine...")
# Menghitung matriks similaritas cosine antara semua film berdasarkan vektor TF-IDF mereka.
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

print("\nUkuran matriks similaritas cosine:", cosine_sim.shape)
print("-" * 30)

"""1. **Menentukan Kesamaan Konten Antar Film:**

 - Kode ini mengambil `tfidf_matrix` sebagai input. `tfidf_matrix` adalah representasi numerik dari konten setiap film (yang telah diolah dari `overview`, `genres`, `keywords`, `cast`, dan `crew`).
 - Dengan menghitung cosine similarity antara setiap pasang vektor TF-IDF film, kode ini bertujuan untuk mengukur seberapa mirip dua film satu sama lain berdasarkan isi teks mereka.
2. **Membentuk Fondasi untuk Rekomendasi Berbasis Konten:**
 - Matriks `cosine_sim` yang dihasilkan adalah komponen kunci dalam sistem rekomendasi berbasis konten. Setelah matriks ini terbentuk, sistem dapat:
    - Menemukan film yang paling mirip dengan film tertentu (misalnya, film yang sedang ditonton pengguna).
    - Merekomendasikan film-film yang memiliki skor kesamaan tinggi dengan film yang sudah disukai pengguna.
"""

# --- Tahap 7: Membuat Fungsi Rekomendasi ---
print("Tahap 7: Membuat Fungsi Rekomendasi...")

# Membuat mapping antara judul film dan indeksnya di DataFrame
# Ini berguna untuk mencari film berdasarkan judul dan mendapatkan skor similaritasnya.
# Reset index data agar sesuai dengan indeks matriks cosine_sim
data = data.reset_index(drop=True)
indices = pd.Series(data.index, index=data['title'])

"""- tujuan dari kode ini adalah untuk membangun "kamus" atau "peta" yang efisien antara judul film dan posisi indeks numeriknya, setelah memastikan bahwa indeks data itu sendiri berurutan dan konsisten."""

def get_recommendations(title, cosine_sim_matrix=cosine_sim, df=data, movie_indices=indices, top_n=10):
    """
    Fungsi untuk mendapatkan rekomendasi film berdasarkan similaritas konten.
    """
    # Mendapatkan indeks film yang cocok dengan judul
    try:
        idx = movie_indices[title]
    except KeyError:
        # Jika judul tidak ditemukan persis, coba cari yang mirip
        similar_titles = [t for t in movie_indices.index if title.lower() in t.lower()]
        if not similar_titles:
            return f"Film dengan judul yang mengandung '{title}' tidak ditemukan."
        elif len(similar_titles) > 1:
            print(f"Ditemukan beberapa judul yang mirip dengan '{title}': {similar_titles}. Menggunakan '{similar_titles[0]}'.")
            idx = movie_indices[similar_titles[0]]
        else:
            idx = movie_indices[similar_titles[0]]


    # Jika ada beberapa film dengan judul yang sama (setelah pencarian parsial), ambil yang pertama
    if isinstance(idx, pd.Series):
        idx = idx.iloc[0]

    # Mendapatkan skor similaritas dari semua film dengan film yang diberikan
    sim_scores = list(enumerate(cosine_sim_matrix[idx]))

    # Mengurutkan film berdasarkan skor similaritas
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Mendapatkan skor dari N film paling mirip (tidak termasuk film itu sendiri)
    sim_scores = sim_scores[1:(top_n + 1)]

    # Mendapatkan indeks film dari skor yang sudah diurutkan
    movie_indices_rec = [i[0] for i in sim_scores]

    # Mengembalikan judul dari N film paling mirip beserta skor similaritasnya
    recommended_movies = df['title'].iloc[movie_indices_rec]
    similarity_scores_rec = [round(s[1], 3) for s in sim_scores]

    return pd.DataFrame({'Judul Film Rekomendasi': recommended_movies, 'Skor Similaritas': similarity_scores_rec})

print("Fungsi rekomendasi siap digunakan.")
print("-" * 30)

"""1. **Definisi Fungsi `get_recommendations`:**
  - Fungsi ini dirancang untuk menerima:
    - `title`: Judul film yang ingin dicari rekomendasinya.
    - `cosine_sim_matrix`: Matriks similaritas kosinus (cosine_sim) yang telah dihitung sebelumnya.
    - `df`: DataFrame utama yang berisi informasi film (terutama untuk mendapatkan judul film).
    - `movie_indices`: Pandas Series yang memetakan judul film ke indeks numeriknya.
    - `top_n`: Jumlah rekomendasi film yang diinginkan (default 10).

2. **Mendapatkan Indeks Film dari Judul (Pencarian & Penanganan Error):**

 - **Tujuan:** Menemukan indeks numerik dari film yang judulnya diberikan. Ini sangat penting karena perhitungan similaritas dan matriks `cosine_sim` bekerja dengan indeks numerik, bukan judul.

3. **Mendapatkan Skor Similaritas:**
 - Tujuan:** Mengambil baris dari `cosine_sim_matrix` yang sesuai dengan film yang diberikan (idx). Baris ini berisi skor similaritas film yang diberikan dengan setiap film lainnya dalam dataset.

4. **Mengurutkan Skor Similaritas:**
 - Tujuan: Mengurutkan daftar `sim_scores` dari skor similaritas tertinggi ke terendah. Ini memastikan bahwa film yang paling mirip akan muncul di bagian atas daftar.

# Mendapatkan rekomendasi
"""

# --- Tahap 8: Mendapatkan Rekomendasi ---
print("Tahap 8: Mendapatkan Rekomendasi...")

# Contoh penggunaan:
movie_title_to_recommend = 'Avatar'
recommendations = get_recommendations(movie_title_to_recommend)
print(f"\nRekomendasi untuk '{movie_title_to_recommend}':")
if isinstance(recommendations, pd.DataFrame):
    print(recommendations)
else:
    print(recommendations) # Jika film tidak ditemukan

print("-" * 30)

"""- kode diatas berfungsi untuk mendapatkan rekomendasi film  dan skor similaritasnya

# Inference Rekomendasi
"""

movie_title_to_recommend_2 = 'The Dark Knight Rises'
recommendations_2 = get_recommendations(movie_title_to_recommend_2)
print(f"\nRekomendasi untuk '{movie_title_to_recommend_2}':")
if isinstance(recommendations_2, pd.DataFrame):
    print(recommendations_2)
else:
    print(recommendations_2)

print("-" * 30)

"""- kode diatas untuk menguji model development berdasarkan judul movienya"""

# Contoh dengan judul yang mungkin tidak persis
movie_title_to_recommend_3 = 'batman'
recommendations_3 = get_recommendations(movie_title_to_recommend_3)
print(f"\nRekomendasi untuk pencarian '{movie_title_to_recommend_3}':")
if isinstance(recommendations_3, pd.DataFrame):
    print(recommendations_3)
else:
    print(recommendations_3)

"""- kode diatas untuk memastikan model bekerja dengan baik berdasarkan judul movienya

# Evaluasi model
"""

eval_df_source = df[['movie_id', 'title', 'genres']].copy()
eval_df_source = eval_df_source.reset_index(drop=True)
def get_primary_genre(genres_list):
    """Extracts the first genre from a list of genres."""
    if genres_list and isinstance(genres_list, list) and len(genres_list) > 0:
        return genres_list[0]
    return None

eval_df_source['primary_genre'] = eval_df_source['genres'].apply(get_primary_genre)


if not eval_df_source['title'].equals(data['title']):
    print("Warning: Titles in eval_df_source and 'data' (final_df) do not perfectly align. Evaluation might be affected.")
    print(f"eval_df_source shape: {eval_df_source.shape}, data shape: {data.shape}")


print("Sample of eval_df_source with primary genre:")
print(eval_df_source[['title', 'genres', 'primary_genre']].head())
print(f"\nNumber of movies with a primary genre: {eval_df_source['primary_genre'].notna().sum()}")
print(f"Number of movies without a primary genre: {eval_df_source['primary_genre'].isna().sum()}")
print("-" * 30)

"""- Menciptakan sebuah dataset (eval_df_source) yang bersih dan terstruktur, dengan setiap film memiliki label "genre utama". Label ini krusial untuk tahap evaluasi, di mana sistem akan menilai seberapa baik rekomendasi yang diberikan cocok dengan genre utama film input. Validasi keselarasan data juga dilakukan untuk memastikan integritas evaluasi."""

def calculate_metrics_for_movie(input_movie_title, recommendations_df, eval_dataframe_with_genres,
                                movie_indices_map, K=10):
    try:
        input_movie_idx = movie_indices_map[input_movie_title]
        if isinstance(input_movie_idx, pd.Series): # Handle if title maps to multiple indices (e.g. duplicates)
            input_movie_idx = input_movie_idx.iloc[0]
    except KeyError:
        # This case should ideally not happen if titles are sampled from 'data'
        # print(f"Input movie '{input_movie_title}' not found in movie_indices_map for evaluation.")
        return 0.0, 0.0

    input_movie_primary_genre = eval_dataframe_with_genres['primary_genre'].iloc[input_movie_idx]

    if pd.isna(input_movie_primary_genre):
        # print(f"Input movie '{input_movie_title}' has no primary genre. Skipping evaluation for this movie.")
        return 0.0, 0.0 # Cannot evaluate if input movie has no primary genre

    if not isinstance(recommendations_df, pd.DataFrame) or recommendations_df.empty:
        return 0.0, 0.0 # No recommendations to evaluate

    recommended_titles = recommendations_df['Judul Film Rekomendasi'].tolist()
    relevant_recommendations_count = 0

    for rec_title in recommended_titles:
        try:
            rec_movie_idx = movie_indices_map[rec_title]
            if isinstance(rec_movie_idx, pd.Series):
                rec_movie_idx = rec_movie_idx.iloc[0]

            rec_movie_primary_genre = eval_dataframe_with_genres['primary_genre'].iloc[rec_movie_idx]
            if rec_movie_primary_genre == input_movie_primary_genre:
                relevant_recommendations_count += 1
        except KeyError:
            # Recommended movie title not in map, cannot determine its genre
            continue
        except IndexError:
            # Index out of bounds, should not happen if map is correct
            continue

    precision_at_k = relevant_recommendations_count / K if K > 0 else 0.0

    # For Recall@K: Total number of *other* movies in the dataset with the same primary genre
    total_relevant_items_in_dataset = eval_dataframe_with_genres[
        eval_dataframe_with_genres['primary_genre'] == input_movie_primary_genre
    ]['title'].count()

    # Subtract 1 because the input movie itself is part of this count
    # and we are interested in recalling *other* relevant movies.
    if total_relevant_items_in_dataset > 0:
        total_relevant_items_in_dataset = max(0, total_relevant_items_in_dataset - 1)

    if total_relevant_items_in_dataset == 0:
        # If there are no other relevant items, recall is 0 (unless relevant_recommendations_count is also 0).
        # Or, if relevant_recommendations_count > 0 but no other relevant items exist, it's an anomaly or perfect recall on a tiny set.
        # Conventionally, if denominator is 0, recall is 0.
        recall_at_k = 0.0 if relevant_recommendations_count == 0 else 1.0 # Or specific handling
        if relevant_recommendations_count > 0 and total_relevant_items_in_dataset == 0: # Edge case: recommended relevant items but no "other" relevant items.
             recall_at_k = 1.0 # All "other" (zero) relevant items were found.

    else:
        recall_at_k = relevant_recommendations_count / total_relevant_items_in_dataset

    return precision_at_k, recall_at_k

print("Evaluation function calculate_metrics_for_movie() is ready.")
print("-" * 30)

"""- Mengukur seberapa baik sistem rekomendasi menghasilkan film yang relevan secara genre dengan film input menggunakan dua metrik:

  - Precision@K: proporsi film dalam top-K rekomendasi yang memiliki genre yang sama dengan film input.

  - Recall@K: proporsi dari seluruh film lain dengan genre yang sama yang berhasil direkomendasikan.
"""

K_for_evaluation = 10  # Evaluate Precision@10 and Recall@10
evaluation_sample_size = 100 # Number of movies to sample for evaluation

if len(data) < evaluation_sample_size:
    evaluation_sample_size = len(data) # Adjust if dataset is smaller

# Sample movies for evaluation from 'data' to ensure they exist and have recommendations.
# Using titles from 'data' ensures they are in the 'indices' map.
sample_movie_titles = data['title'].sample(n=evaluation_sample_size, random_state=42).tolist()

all_precisions = []
all_recalls = []

print(f"Starting evaluation for {len(sample_movie_titles)} movies...\n")
for i, movie_title in enumerate(sample_movie_titles):
    # Get recommendations for the sampled movie_title
    # get_recommendations returns a DataFrame or a string if movie not found
    recommendations = get_recommendations(movie_title, cosine_sim_matrix=cosine_sim, df=data, movie_indices=indices, top_n=K_for_evaluation)

    if isinstance(recommendations, pd.DataFrame):
        # The 'indices' series maps titles to indices in 'data'.
        # 'eval_df_source' is aligned with 'data', so 'indices' can be used for it too.
        precision, recall = calculate_metrics_for_movie(movie_title, recommendations, eval_df_source, indices, K=K_for_evaluation)
        all_precisions.append(precision)
        all_recalls.append(recall)
    # else: movie not found or no recommendations, already handled in calculate_metrics_for_movie or implicitly skipped if recommendations is not DataFrame

    if (i + 1) % (evaluation_sample_size // 10 or 1) == 0: # Print progress
        print(f"Processed {i+1}/{len(sample_movie_titles)} movies for evaluation.")

# Calculate average metrics
average_precision_at_k = np.mean([p for p in all_precisions if not np.isnan(p)]) if all_precisions else 0.0
average_recall_at_k = np.mean([r for r in all_recalls if not np.isnan(r)]) if all_recalls else 0.0

print("-" * 30)
print(f"Evaluation Results (for K={K_for_evaluation} over {len(all_precisions)} successfully evaluated movies):")
print(f"Average Precision@{K_for_evaluation}: {average_precision_at_k:.4f}")
print(f"Average Recall@{K_for_evaluation}: {average_recall_at_k:.4f}")
print("-" * 30)

"""Menghitung rata-rata Precision@10 dan Recall@10 dari sistem rekomendasi dengan:

- Sampling 100 film acak dari dataset (data)

- Mendapatkan rekomendasi untuk tiap film menggunakan get_recommendations()

- Mengukur relevansi rekomendasi menggunakan calculate_metrics_for_movie()

- Menyajikan rata-rata hasil evaluasi


"""

